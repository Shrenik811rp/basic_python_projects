{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import mediapipe as mp\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import uuid \r\n",
    "import os\r\n",
    "\r\n",
    "print(\"all modules imported\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "all modules imported\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#setup the drawing utilities\r\n",
    "#\r\n",
    "\r\n",
    "#draw landmarks on the hands \r\n",
    "# draws joint points when hand is detected\r\n",
    "mp_draw = mp.solutions.drawing_utils\r\n",
    "# #detect hand and draw the hand landmarks\r\n",
    "mp_hand = mp.solutions.hands"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#capture video feed\r\n",
    "capture_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\r\n",
    "\r\n",
    "#instantiating mediapipe model\r\n",
    "#min_detection_confidence = 90% \r\n",
    "#we want the initial detection confidence to be 90\r\n",
    "#This sets the accuracy of model\r\n",
    "# min_tracking_confidence = 60%\r\n",
    "with mp_hand.Hands(min_detection_confidence=0.8,\\\r\n",
    "    min_tracking_confidence=0.8) as hands:\r\n",
    "    #while webcam is capturing\r\n",
    "    while capture_video.isOpened():\r\n",
    "\r\n",
    "        # returns the results of video from each frame\r\n",
    "        ret,frame = capture_video.read()\r\n",
    "\r\n",
    "        #convert bgr to rgb so to work with mediapipe \r\n",
    "        #convert the image to rgb\r\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\r\n",
    "\r\n",
    "        #flip image on horizontal axis\r\n",
    "        image = cv2.flip(image,1)\r\n",
    "\r\n",
    "        #setting a flag that prevent copying of image\r\n",
    "        #prevent overlaying of video detections\r\n",
    "        #cannot draw any landmarks on hand\r\n",
    "        image.flags.writeable = False\r\n",
    "\r\n",
    "\r\n",
    "        #makes the hand detections from video feed\r\n",
    "        #So we process the video frame\r\n",
    "        results = hands.process(image)\r\n",
    "\r\n",
    "        #let us to draw landmarks on hand\r\n",
    "        image.flags.writeable = True\r\n",
    "\r\n",
    "        # convert back from RGB 2 BGR\r\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "        \r\n",
    "        # Detections\r\n",
    "        print(results)\r\n",
    "\r\n",
    "        #Checking if we got any results that passed\r\n",
    "        #Passes the accuracy requirements\r\n",
    "        if results.multi_hand_landmarks:\r\n",
    "\r\n",
    "            #looping through each result of landmarks\r\n",
    "            for num,hand in enumerate(results.multi_hand_landmarks):\r\n",
    "                #We are going to draw landmarks on image\r\n",
    "                mp_draw.draw_landmarks(image,hand,mp_hand.HAND_CONNECTIONS,\\\r\n",
    "                    mp_draw.DrawingSpec(color=(252, 61,3),\\\r\n",
    "                        thickness=2,\r\n",
    "                        circle_radius=2),\r\n",
    "                    mp_draw.DrawingSpec(color=(3, 3, 252),\\\r\n",
    "                        thickness=2,\r\n",
    "                        circle_radius=2))\r\n",
    "\r\n",
    "        #display video feed in window gui \r\n",
    "        cv2.imshow(\"Hand Tracking\",image)\r\n",
    "\r\n",
    "        #to quit press \"q\"\r\n",
    "        # wait for 10 sec\r\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\r\n",
    "            break\r\n",
    "\r\n",
    "# closes the window\r\n",
    "capture_video.release()\r\n",
    "\r\n",
    "# destroy or close the gui window\r\n",
    "cv2.destroyAllWindows()\r\n",
    "print(\"window closed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "window closed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "results.multi_hand_landmarks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "os.mkdir('Hand Pose Images')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#capture video feed\r\n",
    "capture_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\r\n",
    "\r\n",
    "#instantiating mediapipe model\r\n",
    "#min_detection_confidence = 90% \r\n",
    "#we want the initial detection confidence to be 90\r\n",
    "#This sets the accuracy of model\r\n",
    "# min_tracking_confidence = 60%\r\n",
    "with mp_hand.Hands(min_detection_confidence=0.8,\\\r\n",
    "    min_tracking_confidence=0.8) as hands:\r\n",
    "    #while webcam is capturing\r\n",
    "    while capture_video.isOpened():\r\n",
    "\r\n",
    "        # returns the results of video from each frame\r\n",
    "        ret,frame = capture_video.read()\r\n",
    "\r\n",
    "        #convert bgr to rgb so to work with mediapipe \r\n",
    "        #convert the image to rgb\r\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\r\n",
    "\r\n",
    "        #flip image on horizontal axis\r\n",
    "        image = cv2.flip(image,1)\r\n",
    "\r\n",
    "        #setting a flag that prevent copying of image\r\n",
    "        #prevent overlaying of video detections\r\n",
    "        #cannot draw any landmarks on hand\r\n",
    "        image.flags.writeable = False\r\n",
    "\r\n",
    "\r\n",
    "        #makes the hand detections from video feed\r\n",
    "        #So we process the video frame\r\n",
    "        results = hands.process(image)\r\n",
    "\r\n",
    "        #let us to draw landmarks on hand\r\n",
    "        image.flags.writeable = True\r\n",
    "\r\n",
    "        # convert back from RGB 2 BGR\r\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "        \r\n",
    "        # Detections\r\n",
    "        print(results)\r\n",
    "\r\n",
    "        #Checking if we got any results that passed\r\n",
    "        #Passes the accuracy requirements\r\n",
    "        if results.multi_hand_landmarks:\r\n",
    "\r\n",
    "            #looping through each result of landmarks\r\n",
    "            for num,hand in enumerate(results.multi_hand_landmarks):\r\n",
    "                #We are going to draw landmarks on image\r\n",
    "                mp_draw.draw_landmarks(image,hand,mp_hand.HAND_CONNECTIONS,\\\r\n",
    "                    mp_draw.DrawingSpec(color=(252, 61,3),\\\r\n",
    "                        thickness=2,\r\n",
    "                        circle_radius=2),\r\n",
    "                    mp_draw.DrawingSpec(color=(3, 3, 252),\\\r\n",
    "                        thickness=2,\r\n",
    "                        circle_radius=2))\r\n",
    "\r\n",
    "        #save the image as a photo\r\n",
    "        #write out image that is captured\r\n",
    "        cv2.imwrite(\r\n",
    "            os.path.join(\r\n",
    "\r\n",
    "                #create a path where image will be stored\r\n",
    "                #image stored in \"Hand Pose Images\" folder\r\n",
    "                \"Hand Pose Images\",\r\n",
    "                #naming image with unique formatting id\r\n",
    "                #append .jpg to unique name\r\n",
    "        '{}.jpg'.format(uuid.uuid1())),\r\n",
    "         image)\r\n",
    "\r\n",
    "        #display video feed in window gui \r\n",
    "        cv2.imshow(\"Hand Tracking\",image)\r\n",
    "\r\n",
    "        #to quit press \"q\"\r\n",
    "        # wait for 10 sec\r\n",
    "        if cv2.waitKey(3000) & 0xFF == ord('q'):\r\n",
    "            break\r\n",
    "\r\n",
    "# closes the window\r\n",
    "capture_video.release()\r\n",
    "\r\n",
    "# destroy or close the gui window\r\n",
    "cv2.destroyAllWindows()\r\n",
    "print(\"window closed\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "window closed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('hand': venv)"
  },
  "interpreter": {
   "hash": "66437ed558f750529b5e8b18238a4c8dde978156669cf28d7ba9b1a04bff532e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}